---
title: 博搜?-开发总结
tags:
  - crawler
  - elasticsearch
  - python
  - snippet
url: 1183.html
id: 1183
categories:
  - DEV
date: 2019-09-15 22:24:02
---

X真正的去做一个东西出来，难免会踩不少坑。 有问题不可怕，记下来，解决它。

先mark一下自己的小成果吧，成绩还不错，差不多两天的时间，从平地起了个小房屋。

> [https://search.12ms.xyz/v2](https://search.12ms.xyz/v2)

名字没想好，既然是专门搜博客的，就先称之为博搜了。虽然名字不好听。 这一篇博文主要是记录在开发过程中的遇到的各种坑坑洼洼坎坎坷坷。以及一些小技巧，日后的话可以快速的用起来。

文本处理类的
------

这一部分写和文本相关的一些问题和技巧

### 提取repr(e)的错误信息

由于RSS的格式各部一样，所以在解析过程中很容易出现AttrError，所以想提取错误信息自动处理，提取也就是第一步，内容是使用 `' '` 单引号扩起来的，所以很容易联想到使用正则来提取数据。

    err_item = re.compile("'(.*)'").findall(repr(e))[0]

### 去除字符串中的html标签

由于需要录入feed的信息，但是如果有html的标签在的话，在页面上显示出来仰望着被解析，导致页面混乱。所以需要去掉字符串中的html标签，一样使用正则。

    pure_str = re.compile(r'<[^>]+>',re.S).sub('',html_str)

爬虫
--

自己手动收集rss的速度太慢了，那就自动化吧，来上一个爬虫。

### 请求头伪装

里面很重要的一步是请求头伪装，避免一些过滤策略，导致的 4xx

    headers = {
        'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
    }

* * *

遇到一个很不错的系列，这里也贴出来：

> [https://github.com/brandonxiang/example-requests](https://github.com/brandonxiang/example-requests)

es相关内容
------

这一部分单的的拆出来：

> [ES 搜索语法收集](https://blog.12ms.xyz/2019/09/21/es-%e6%90%9c%e7%b4%a2%e6%8a%80%e5%b7%a7/)

代码片
---

这里作为代码段整理

### Flask 的restful 捕获 /xxx/*

由于需要设计一个 类restful 的接口，所以这里需要对路由进行通配的捕获，在flask中的修饰头如下：

    @app.route('/concenter/', defaults={'path': ''},methods=['GET', 'POST', 'PUT', 'DELETE'])
    @app.route('/concenter/',methods=['GET', 'POST', 'PUT', 'DELETE'])
    def req_handler(path):

这样就会捕获后面的任意一个URL作为 path 传入。

> 参考：[https://codeday.me/bug/20180704/187717.html](https://codeday.me/bug/20180704/187717.html)

### 多线程

一段漂亮的多线程代码，在批量导入的时候使用的：

    import queue
    import threading
    import time
    
    class myThread (threading.Thread):
        def __init__(self, threadID, name, q):
            threading.Thread.__init__(self)
            self.threadID = threadID
            self.name = name
            self.q = q
    
        def process_data(self, threadName, q):
            while not exitFlag:
                queueLock.acquire()
                if not workQueue.empty():
                    data = q.get()
                    queueLock.release()
                    rss2es(data)
                    print("%s processing %s" % (threadName, data))
                else:
                    queueLock.release()
                time.sleep(1)
    
        def run(self):
                print("Starting " + self.name)
                self.process_data(self.name, self.q)
                print("Exiting " + self.name)
    
    exitFlag = 0
    
    if __name__ == "__main__":
        queueLock = threading.Lock()
        workQueue = queue.Queue()
        threads = []
        threadID = 1
    
        # 创建新线程
        for tName in threadList:
            thread = myThread(threadID, tName, workQueue)
            thread.start()
            threads.append(thread)
            threadID += 1
    
        # 填充队列
        queueLock.acquire()
        for word in nameList:
            workQueue.put(word)
        queueLock.release()
    
        # 等待队列清空
        while not workQueue.empty():
            pass
    
        # 通知线程是时候退出
        exitFlag = 1
        for t in threads:
            t.join()
        print("Exiting Main Thread")
    

使用了queue的队列加锁来实现线程之间的同步。

### 获取本机IP

    import socket
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(('8.8.8.8', 80))
            ip = s.getsockname()[0]
        finally:
            s.close()
        return ip

获取可用IP 的好方法就是去建立一个连接，当然这个只是联机可用。

问题
--

### python 中的 **init**.py

当一个 python项目 的扩大，需要进行目录的分化，这里就需要使用到了python 的包，来对项目来进行拆分。